{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99506c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import pillow_avif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53081cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "image_path = r\"C:\\Users\\jodis\\ML\\Project3\\Images\"\n",
    "\n",
    "print(\"Image root path:\", image_path)\n",
    "print(\"Does the path exist?\", os.path.exists(image_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56362a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert type and transform image size\n",
    "\n",
    "valid_extensions = {\".avif\", \".png\", \".webp\", \".jpg\", \".jpeg\"}\n",
    "\n",
    "for folder, subfolders, files in os.walk(image_path):\n",
    "    for file in files:\n",
    "        file_lower = file.lower()\n",
    "        file_ext = os.path.splitext(file_lower)[1]\n",
    "\n",
    "        if file_ext in valid_extensions:\n",
    "            full_path = os.path.join(folder, file)\n",
    "\n",
    "            # New filename with .jpg extension\n",
    "            new_filename = os.path.splitext(file)[0] + \".jpg\"\n",
    "            new_path = os.path.join(folder, new_filename)\n",
    "\n",
    "            # If the .jpg already exists, skip to avoid redoing work\n",
    "            if os.path.exists(new_path):\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                img = Image.open(full_path).convert(\"RGB\")\n",
    "                img.save(new_path, \"JPEG\", quality=95)\n",
    "                print(f\"Converted: {full_path} -> {new_path}\")\n",
    "\n",
    "                # Delete original file only after successful save\n",
    "                if full_path != new_path:\n",
    "                    os.remove(full_path)\n",
    "                    print(f\"Deleted original file: {full_path}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to convert {full_path}: {e}\")\n",
    "\n",
    "print(\"Image conversion step completed.\\n\")\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.Resize((500, 500)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "full_dataset = datasets.ImageFolder(\n",
    "    root=image_path,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "print(\"Classes found:\", full_dataset.classes)\n",
    "print(\"Total images:\", len(full_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f21b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/valid/test split\n",
    "torch.manual_seed(1)\n",
    "\n",
    "dataset_size = len(full_dataset)\n",
    "valid_size = int(0.2 * dataset_size)\n",
    "test_size = int(0.1 * dataset_size)\n",
    "train_size = dataset_size - valid_size - test_size\n",
    "\n",
    "train_dataset, valid_dataset, test_dataset = random_split(full_dataset, [train_size, valid_size, test_size])\n",
    "\n",
    "print(f\"Train size: {train_size}\")\n",
    "print(f\"Validation size: {valid_size}\")\n",
    "print(f\"Test size: {test_size}\")\n",
    "\n",
    "# DataLoaders\n",
    "batch_size = 32\n",
    "train_dl = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_dl = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_dl = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72fa34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN architecture\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "num_classes = len(full_dataset.classes)\n",
    "print(\"Number of classes:\", num_classes)\n",
    "\n",
    "def build_cnn(dropout_p=0.5):\n",
    "    model = nn.Sequential()\n",
    "\n",
    "    # Block 1\n",
    "    model.add_module('conv1', nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1))\n",
    "    model.add_module('relu1', nn.ReLU())\n",
    "    model.add_module('pool1', nn.MaxPool2d(kernel_size=2))      # 500 -> 250\n",
    "    model.add_module('dropout1', nn.Dropout(p=dropout_p))\n",
    "\n",
    "    # Block 2\n",
    "    model.add_module('conv2', nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1))\n",
    "    model.add_module('relu2', nn.ReLU())\n",
    "    model.add_module('pool2', nn.MaxPool2d(kernel_size=2))      # 250 -> 125\n",
    "    model.add_module('dropout2', nn.Dropout(p=dropout_p))\n",
    "\n",
    "    # Block 3\n",
    "    model.add_module('conv3', nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1))\n",
    "    model.add_module('relu3', nn.ReLU())\n",
    "    model.add_module('pool3', nn.MaxPool2d(kernel_size=2))      # 125 -> 62 (floor)\n",
    "\n",
    "    # Block 4\n",
    "    model.add_module('conv4', nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1))\n",
    "    model.add_module('relu4', nn.ReLU())\n",
    "    model.add_module('pool4', nn.MaxPool2d(kernel_size=2))      # 62 -> 31\n",
    "\n",
    "    # Global average pooling: 256 x 31 x 31 -> 256 x 1 x 1\n",
    "    model.add_module('gap', nn.AdaptiveAvgPool2d((1, 1)))\n",
    "    model.add_module('flatten', nn.Flatten())\n",
    "\n",
    "    # Fully connected layer to num_classes\n",
    "    model.add_module('fc', nn.Linear(256, num_classes))\n",
    "\n",
    "    return model.to(device)\n",
    "\n",
    "# Check forward pass shape\n",
    "tmp_model = build_cnn(dropout_p=0.5)\n",
    "x_dummy = torch.ones((4, 3, 500, 500)).to(device)\n",
    "with torch.no_grad():\n",
    "    out_dummy = tmp_model(x_dummy)\n",
    "print(\"Output shape (batch_size=4):\", out_dummy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad98f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_model(model, num_epochs, train_dl, valid_dl, lr=0.001):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    loss_hist_train = [0.0] * num_epochs\n",
    "    accuracy_hist_train = [0.0] * num_epochs\n",
    "    loss_hist_valid = [0.0] * num_epochs\n",
    "    accuracy_hist_valid = [0.0] * num_epochs\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Train\n",
    "        model.train()\n",
    "        for x_batch, y_batch in train_dl:\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            pred = model(x_batch) # shape: [batch_size, num_classes]\n",
    "            loss = loss_fn(pred, y_batch)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss_hist_train[epoch] += loss.item() * y_batch.size(0)\n",
    "            is_correct = (torch.argmax(pred, dim=1) == y_batch).float()\n",
    "            accuracy_hist_train[epoch] += is_correct.sum().item()\n",
    "\n",
    "        loss_hist_train[epoch] /= len(train_dl.dataset)\n",
    "        accuracy_hist_train[epoch] /= len(train_dl.dataset)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for x_batch, y_batch in valid_dl:\n",
    "                x_batch = x_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "\n",
    "                pred = model(x_batch)\n",
    "                loss = loss_fn(pred, y_batch)\n",
    "\n",
    "                loss_hist_valid[epoch] += loss.item() * y_batch.size(0)\n",
    "                is_correct = (torch.argmax(pred, dim=1) == y_batch).float()\n",
    "                accuracy_hist_valid[epoch] += is_correct.sum().item()\n",
    "\n",
    "        loss_hist_valid[epoch] /= len(valid_dl.dataset)\n",
    "        accuracy_hist_valid[epoch] /= len(valid_dl.dataset)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"Train_acc: {accuracy_hist_train[epoch]:.4f}\")\n",
    "        print(f\"Val_acc: {accuracy_hist_valid[epoch]:.4f}\")\n",
    "\n",
    "    return loss_hist_train, loss_hist_valid, accuracy_hist_train, accuracy_hist_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b805053f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning (multiple candidate CNNs)\n",
    "torch.manual_seed(1)\n",
    "\n",
    "candidate_configs = [\n",
    "    {\"name\": \"model_lr_1e-3_drop_0.5\", \"lr\": 1e-3, \"dropout\": 0.5},\n",
    "    {\"name\": \"model_lr_5e-4_drop_0.5\", \"lr\": 5e-4, \"dropout\": 0.5},\n",
    "    {\"name\": \"model_lr_1e-3_drop_0.3\", \"lr\": 1e-3, \"dropout\": 0.3},\n",
    "    {\"name\": \"model_lr_5e-4_drop_0.3\", \"lr\": 5e-4, \"dropout\": 0.3},\n",
    "    {\"name\": \"model_lr_1e-4_drop_0.3\", \"lr\": 1e-4, \"dropout\": 0.3},\n",
    "]\n",
    "\n",
    "num_epochs_tune = 10\n",
    "\n",
    "history_by_name = {}\n",
    "best_val_acc = 0.0\n",
    "best_config = None\n",
    "best_model_state = None\n",
    "\n",
    "for config in candidate_configs:\n",
    "    print(f\"Training candidate: {config['name']}\")\n",
    "\n",
    "    # Build a fresh model for this config\n",
    "    model = build_cnn(dropout_p=config[\"dropout\"])\n",
    "\n",
    "       # Train model\n",
    "    hist = train_model(\n",
    "        model=model,\n",
    "        num_epochs=num_epochs_tune,\n",
    "        train_dl=train_dl,\n",
    "        valid_dl=valid_dl,\n",
    "        lr=config[\"lr\"]\n",
    "    )\n",
    "\n",
    "    history_by_name[config[\"name\"]] = hist\n",
    "\n",
    "    # Use final validation accuracy as score\n",
    "    val_acc_best_epoch = max(hist[3])\n",
    "    print(f\"Best validation accuracy for {config['name']}: {val_acc_best_epoch:.4f}\")\n",
    "\n",
    "    if val_acc_best_epoch > best_val_acc:\n",
    "        best_val_acc = val_acc_best_epoch\n",
    "        best_config = config\n",
    "        best_model_state = model.state_dict()\n",
    "\n",
    "print(\"\\nBest candidate configuration:\", best_config)\n",
    "print(f\"Best validation accuracy: {best_val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e32f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rebuild best model and fine tune\n",
    "final_model = build_cnn(dropout_p=best_config[\"dropout\"])\n",
    "final_model.load_state_dict(best_model_state)\n",
    "\n",
    "# Optionally, train a few more epochs on the same train/valid split\n",
    "num_epochs_final = 0  # set > 0 if you want further training\n",
    "\n",
    "if num_epochs_final > 0:\n",
    "    print(\"\\nFine-tuning best model further...\")\n",
    "    _ = train_model(\n",
    "        model=final_model,\n",
    "        num_epochs=num_epochs_final,\n",
    "        train_dl=train_dl,\n",
    "        valid_dl=valid_dl,\n",
    "        lr=best_config[\"lr\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c961b754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test-set evaluation\n",
    "final_model.eval()\n",
    "test_correct = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x_batch, y_batch in test_dl:\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        pred = final_model(x_batch)\n",
    "        predicted_labels = torch.argmax(pred, dim=1)\n",
    "\n",
    "        is_correct = (predicted_labels == y_batch).float()\n",
    "        test_correct += is_correct.sum().item()\n",
    "\n",
    "test_accuracy = test_correct / len(test_dl.dataset)\n",
    "print(f\"Test accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24670910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "best_name = best_config[\"name\"]\n",
    "best_hist = history_by_name[best_name]\n",
    "\n",
    "loss_hist_train, loss_hist_valid, acc_hist_train, acc_hist_valid = best_hist\n",
    "\n",
    "epochs_arr = np.arange(len(loss_hist_train)) + 1\n",
    "\n",
    "fig = plt.figure(figsize=(12, 4))\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "ax.plot(epochs_arr, loss_hist_train, '-o', label='Train loss')\n",
    "ax.plot(epochs_arr, loss_hist_valid, '--<', label='Validation loss')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.legend()\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "ax.plot(epochs_arr, acc_hist_train, '-o', label='Train acc.')\n",
    "ax.plot(epochs_arr, acc_hist_valid, '--<', label='Validation acc.')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a157fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "models_dir = \"models\"\n",
    "if not os.path.exists(models_dir):\n",
    "    os.mkdir(models_dir)\n",
    "\n",
    "save_path = os.path.join(models_dir, \"Group_26_CNN_FullModel.ph\")\n",
    "\n",
    "# Save entire model (architecture + weights)\n",
    "torch.save(final_model, save_path)\n",
    "\n",
    "print(\"Saved final model to:\", save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff139008",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "How did you create your dataset and determine split of official logo versus not official logo?\n",
    "\n",
    "I built the dataset by collecting images from google images, including images from the schools website, etsy, and pinterest. I then sorted the images into folders based on whether or not they were official logos or not. The unofficial folder contained other arkansas logos that weren’t the official logo, arkansas merch, and even pictures of pigs. I tried to include various colorways of the official logos, and a good mix of different images in the unofficial folder. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9afbd29",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "Why did you choose the specific architecture for the final model?\n",
    "\n",
    "For our project, we designed a model architecture tailored for binary logo classification to determine whether an image contains the logo or not. We implemented a convolutional neural network (CNN) and used a cross entropy loss function, which directly works with logits. Because the loss function applies the necessary normalization internally, our model does not require a final softmax layer. This setup allows the network to output raw prediction scores while still enabling stable and effective training. The architecture uses multiple convolution and pooling layers to extract increasingly abstract visual features of logos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00548ab3",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "How did you monitor and mitigate overfitting?\n",
    "\n",
    "Overfitting was monitored using training vs. validation accuracy and loss curves, as well as final test accuracy on previously unseen images. The model mitigates overfitting through dropout layers in the CNN, proper train/validation/test splitting, shuffling of training batches, and hyperparameter tuning of learning rate and dropout strength. These combined approaches ensure that the selected model is the one that generalizes best, not just the one that performs well on the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bfbc01",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "What future efforts do you recommend to improve model performance?\n",
    "\n",
    "To improve our project in the future, we could gather a bigger and more diverse dataset of logos and non logo images to strengthen the model’s ability to generalize. Trying more advanced or specialized image recognition architectures might also help boost accuracy. Additionally, fine tuning our model on higher quality images, and removing the backgrounds, could help the model focus. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
